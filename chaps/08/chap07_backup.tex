\chapter{Computational Methods with Sparse Matrices}
\label{chap:sparseMatrix}
{
	\hypersetup{linkcolor=black}
	\minitoc
}
\clearpage 



\underline{\textbf{Computational Methods with Sparse Matrices}}

\section{Introduction}

To better understand the advanced ODE solvers and their connection with
computational linear algebra, we first need to briefly consider the solution
methods underlying stiff systems of ODEs and their corresponding matrix algebra.
In general some classes of multistep methods are used for the solution of a
system of ODEs of size $N \times N$ of the form:
\begin{equation}\label{E:linde}
    \bm{y'}= \frac{d{\bm{y}}}{dt} = {\bm {f}}\ts (t,{\bm{y}}), \quad {\bm{y}}(t_0)= {\bm {a}}
\end{equation}

To advance the solution at time $t$ from one mesh point to the next, considering
a discrete time mesh $\{t_0,t_1,t_2, ...,t_n\}$, multistep methods make use of
several past values of the variable $y$ and its rate of change $f$ with respect
to time $t$. The general form of a \mbox{$k$-step} multistep method is:
\begin{equation}\label{E:linde}
    \sum_{i=0}^k \alpha_i y_{j+i} = h \sum_{i=0}^k \beta_i f_{j+i},
\end{equation}
where $\alpha_k=1$, $\alpha_i$  and $\beta_i$  are constants and depend on the
order of the method, $h$ is the stepsize in time and $j$ denotes the mesh
number. The well-known family of Adams methods:
\begin{equation}\label{E:linde}
    y_{j+k} = y_{j+k-1} + h \sum_{i=0}^{k} \beta_i f_{j+i},
\end{equation}
which use mostly the past values of $f$, are the best-known multistep methods for
solving nonstiff problems. Each step requires the solution of a nonlinear system
and often a simple functional iteration with an initial guess, or a predictor
estimate, is used to advance the integration which is terminated by a
convergence test. For stiff problems, where sudden changes in the variables can
occur (i.e. where there are sharp changes in variables $y$ over small time
intervals say) simple iteration methods lead to an unacceptable restriction of
the stepsize and functional iteration fails to converge. Thus, stiffness forces
the use of implicit methods, with infinite regions of stability, when there is
no restriction on the choice of the stepsize $h$. The backward difference
formulae (BDF) methods with unbounded region of absolute stability were the
first numerical methods developed for solving stiff ODEs (Curtiss and
Hirschfelder, 1952). The BDF methods used in ODE solvers, are of the general
form:
\begin{equation}\label{E:linde}
    \sum_{i=0}^k \alpha_i y_{j+i}= h \beta_k f_{j+k}
\end{equation}
where $\alpha_i$  and $\beta_k$ are coefficients of $k^{th}$ order,
$k \rm -step$ BDF methods.  As noted earlier, a simple functional iteration will
usually fail to converge when problems are stiff and instead some form of Newton
iteration is used for the solution of the resulting nonlinear system. The Newton
iteration involves the solution of an $N \times N$ matrix $P$,
\begin{equation}\label{E:linde}
    P \approx I + h \beta_k J
\end{equation}

where $J = \frac{\partial{f}}{\partial{y}}$ is the associated Jacobian matrix
and $I$ is an $N \times N$ identity matrix.

\newpage
The solution to this linear algebraic system contributes significantly to the
total computational time for the stiff problems, as well as affecting the
accuracy of the solution ( and hence, also affecting computational time).  For
stiff problems the ODE solvers use a modified Newton iteration that allows time
saving strategies for the computation, storage and the use of the Jacobian
matrix.  When solving a linear algebraic system there are generally two classes
of solution methods, direct methods and indirect methods.  The most common
direct method used to solve linear systems is the Gaussian Elimination method
based on factorization of the matrix in lower and upper triangular factors. The
GEAR, LSODE and VODE solvers all use such a method for the solution of the
resulting linear system.  The simplest iterative scheme used to solve linear
systems is the Jacobi iteration, although the more sophisticated solution
methods of Krylov subspace methods, based on a sequence of orthogonal vectors
and matrix-vector multiplications, have been widely used in practical
applications such as computational fluid dynamics.  The ODE solvers, LSODPK and
VODEPK implement preconditioned Krylov iterative techniques for the solution of
the resulting linear system.

\section{Sparse Matrices}

Definitions: A sparse matrix is a matrix with a large number of zero elements,
compared to the total number of elements. Alternatively, we say a matrix can be
considered as sparse if the number of nonzero elements $nnz \ll (N^{2} + N)/2$.

The Newton iteration solution technique introduces a large amount of
computational overhead necessary to solve the linear algebraic system related to
the Jacobian matrix of a stiff system of ODEs. Some matrices have particular
structure that are more convenient for computational purposes, for example upper
triangular matrices, lower triangular matrices and banded matrices.  Ordering
the columns of a matrix can often make its lower and upper (LU) factors sparser.
The simplest such ordering is to sort of columns or rows by nonzero count (i.e
by the nonzero elements).  This is often a good ordering scheme for matrices
with very irregular structure, especially if there is a great variation in the
nonzero counts of rows or columns. Therefore, although by ordering of matrices
the position of nonzero elements will be different and the resulting ordered
matrix will have a completely different structure, this will not change the
physical problem in any way and the reordering will have a significant impact on
the solution techniques. As an example, if Gaussian Elimination method is used
(e.g. in the GEAR, LSODE and VODE solvers) on an ordered matrix, then none or
very few fill-ins (of originally zero entries) will occur in the factorization
processes, i.e. the L and U parts of the LU factorization will have the same
structures as the lower and upper parts of the original matrix, respectively.
Thus, ordering allows the sparsity structure of the system to be preserved in
the factorization processes.

\newpage
\section{Matrix Sparsity and Graph Theory}

A graph $G = \{v,E\}$ consists of a set of vertices $v\text{(nodes)}$ and a
set of edges $E$ connecting the vertices.  A graph can be used to represent the
nonzero pattern of a sparse matrix.  Any symmetric sparse matrix can be
represented by a graph $G$ where there is an edge from vertex
$i\ts\text{(row)}$ to vertex $j\ts\text{(column)}$ and a letter or symbol
represents the nonzero entry, e.g. $a_{ij}$:

Associated with each graph is an adjacency matrix $A = (a_{ij})$:

\begin{equation*}
    (a_{ij})=\begin{cases}
        1, & \text{if an edge exists},\\
        0, & \text{otherwise}.
    \end{cases}
\end{equation*}

The degree of a node in a graph is the number of nodes adjacent to it (or the
number of edges connected to it).
\noindent
\vskip 10pt
\noindent
{\bf\underline {Example 1}}
\vskip 10pt
\noindent
Consider the following matrix $A$ and its corresponding adjacency graph.
\begin{table}[H]
  \begin{minipage}[b]{0.50\linewidth}\centering
    \vskip 10pt
    $A=
    \begin{bmatrix}
      1 & 1 & 0 & 0 & 1 \\
      1 & 1 & 1 & 1 & 0 \\
      0 & 1 & 1 & 1 & 0 \\
      0 & 1 & 1 & 1 & 1 \\
      1 & 0 & 0 & 1 & 1
    \end{bmatrix}$
    \vspace{5mm}
  \end{minipage}
  \begin{minipage}[b]{0.49\linewidth}
    \includegraphics[width=7cm]{main/07/extex_1.tikz}
  \end{minipage}
\end{table}
\vskip7mm
\noindent
The following MATLAB .m file produces the adjacency graph of matrix A:

\noindent
{\small
\begin{lstlisting}
%Example 1 - Adjacency graph using gplot function
A=[1 1 0 0 1;1 1 1 1 0;0 1 1 1 0;0 1 1 1 1;1 0 0 1 1];
% Enter the Cartesian coordinates for the vertices
xy=[1 1; 2 2; 5 2; 4 1; 3 1];
gplot(A,xy,'k-*');% marking the elements using * in black font
axis([0 6 0 3]);
text(1,0.9,'1');
text(2,2.1,'2');
text(5,2.1,'3');
text(4,0.9,'4');
text(3,0.9,'5');		
\end{lstlisting}}

\newpage
{\bf\underline {Example 2}}

\noindent
Consider the following matrix $A$ and its corresponding adjacency graph.
\begin{table}[H]
  \begin{minipage}[b]{0.50\linewidth}\centering
    \vskip -10pt
    $A=
    \begin{bmatrix}
      1 & 1 & 0 & 0 & 1 & 0\\
      1 & 1 & 1 & 0 & 0 & 1\\
      0 & 1 & 1 & 0 & 0 & 0\\
      0 & 0 & 0 & 1 & 1 & 0\\
      1 & 0 & 0 & 1 & 1 & 1\\
      0 & 1 & 0 & 0 & 1 & 1\\
    \end{bmatrix}$
    \vspace{3mm}
  \end{minipage}
  \begin{minipage}[b]{0.49\linewidth}\
    \includegraphics[width=7cm]{main/07/extex_2.tikz}
  \end{minipage}
\end{table}
\vskip -15pt
\noindent
\ttfamily
\begin{lstlisting}
%Example 2 - Adjacency plot
A=[1 1 0 0 1 0;1 1 1 0 0 1;0 1 1 0 0 0;0 0 0 1 1 0;
    1 0 0 1 1 1; 0 1 0 0 1 1];
xy=[2 1;3 1;4 1;1 2;2 2;3 2];
gplot(A,xy,'k-*');
axis([0 5 0 3]);
text(2,0.9,'1');
text(3,0.9,'2');
text(4,0.9,'3');
text(1,2.1,'4');
text(2,2.1,'5');
text(3,2.1,'6');
%End											
\end{lstlisting}

\rmfamily
\noindent
In MATLAB the function $\emph{spy}$ plots the sparsity pattern (i.e. the
non-zero elements) of any matrix A.
\vskip 10pt
\noindent
{\bf\underline {Example 3}}
\vskip 3pt
\begin{table}[H]
  \begin{minipage}[b]{0.50\linewidth}
    \noindent
    Enter the matrix A in MATLAB
    \noindent
    \begin{lstlisting}
>> A=[1 0 2 3 0
      0 0 0 5 0
      0 1 0 0 8
      1 0 0 0 4
      0 0 6 7 0]

Followed by the command spy
>> spy(A)
    \end{lstlisting}
    \vspace{5mm}
    \rmfamily
    \noindent
    MATLAB will produce the sparsity pattern \hfill\break
    \noindent
    (nonzero elements) for matrix $A$ as:
    \vspace{10mm}
  \end{minipage}
  \begin{minipage}[b]{0.49\linewidth}
    %\includegraphics{extex_3.tikz}
      \input{main/07/exp3.tex}
  \end{minipage}
\end{table}
\noindent
\vskip -20pt
The number of nonzero elements in matrix $A$ is $10$ and this is noted by
$nnz=10$ in Fig. 3.

\newpage
\noindent
{\bf\underline {Example 4}}
\vskip 3pt
Define the matrix $A$, from Example 2, in MATLAB, followed by the command
spy(A), gives:

%\begin{table}[H]
  \begin{minipage}[  ]{0.50\linewidth}
  %  \vskip 5pt
    \begin{lstlisting}
>> A=[1 1 0 0 1 0
      1 1 1 0 0 1
      0 1 1 0 0 0
      0 0 0 1 1 0
      1 0 0 1 1 1
      0 1 0 0 1 1]

>> spy(A)
    \end{lstlisting}
  %\vspace{25mm}
  \end{minipage}
  \begin{minipage}[]{0.49\linewidth}
    %\begin{center}
    %  \includegraphics[width=7cm]{main/07/Example4.tikz}
    %\end{center}
    \centering
	\input{main/07/Example4.tikz}
  \end{minipage}
%\end{table}

%\vskip -10pt
%\noindent
%{\bf\underline {3. Sparse Matrices Storage and Reordering}}
%\vskip 5pt
%\noindent
%{\bf\underline {3.1 Storing Sparse Matrices}}
%\vskip 3pt

\section{Sparse Matrices Storage and Reordering}
\subsection{Storing Sparse Matrices}
The coordinate scheme is the most preferred way to specify a sparse matrix in
the form of an unordered set of triples $(i,j, a_{ij})$. Consider the following
example.
\vskip 3pt
\noindent
{\bf\underline {Example 5}}
\vskip 3pt
\noindent
Consider a $5\times5$ sparse matrix A:
\vskip 3pt
\noindent
$
A=
\begin{bmatrix}
1 & 0 & 2 & 3 & 0 \\
0 & 0 & 0 & 5 & 0 \\
0 & 1 & 0 & 0 & 8 \\
1 & 0 & 0 & 0 & 4 \\
0 & 0 & 6 & 7 & 0
\end{bmatrix}
$
\noindent
\vskip 1pt
\noindent
\vskip 1pt
\noindent
where $i$ denotes the row and $j$ denotes the column numbers. Note that the
number of nonzero elements is 10. The coordinate scheme representation of matrix
$A$ is shown in the following table:
\vskip 1pt
\noindent
\begin{center}
    \begin{tabular}{ r | c c c c c c c c c c }
    Subscripts  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    $i$         & 1 & 4 & 3 & 1 & 5 & 1 & 2 & 5 & 3 & 4 \\
    $j$         & 1 & 1 & 2 & 3 & 3 & 4 & 4 & 4 & 5 & 5 \\
    $a_{ij}$    & 1 & 1 & 1 & 2 & 6 & 3 & 5 & 7 & 8 & 4 \\
\end{tabular}
\end{center}
\vskip 4pt
\noindent
From the table, the second column $i = 4$, $j= 1$. This means the matrix element
is 1 in this example.  For sparse matrices, MATLAB uses the same approach to
store the nonzero elements and their indices. The $\emph{sparse}$ function
generates matrices in the MATLAB sparse storage organisation. It converts a full
matrix to sparse form by only considering the nonzero elements.

\newpage
\noindent
{\bf\underline {Example 6}}
\vskip 10pt
\noindent
Consider matrix $A$ in Example 5, with 10 nonzero elements. To convert matrix
$A$ to sparse form in MATLAB use $\emph{sparse}$ function:
\noindent
{\ttfamily
\begin{lstlisting}
>> A=[
1 0 2 3 0
0 0 0 5 0
0 1 0 0 8
1 0 0 0 4
0 0 6 7 0];

>> sparse (A)
ans =
   (1,1)        1
   (4,1)        1
   (3,2)        1
   (1,3)        2
   (5,3)        6
   (1,4)        3
   (2,4)        5
   (5,4)        7
   (3,5)        8
   (4,5)        4
\end{lstlisting}
}
%\vskip 5pt
%\noindent
%{\bf\underline {3.2 Matrix Storage Information}}
\subsection{Matrix Storage Information}
%\vskip 10pt
%\noindent
A matrix can be stored in full or only by its nonzero elements. For example, it
will have the same entries and their elements, eigenvalues and determinants are
equal but the computation storages are different.  In general, the computation
storage of sparse matrices is proportional to the number of nonzero elements.
The $\emph{whos}$ command in MATLAB provides the general information about
matrix size and storage as shown in Example 5.
\vskip 10pt
\noindent
\begin{center}
    \begin{tabular}{ l l l }
        \hline
        Name      & Size     & Bytes Class  \\
        \hline
        Full A    &$5\times5$& 200 double\\
        Sparse A  &$5\times5$& 144 double\\
    \end{tabular}
\end{center}
\vskip 10pt
\noindent
The number of bytes used in the sparse version is less than the full version,
since the zero elements are not stored.  For a large scale sparse matrix system,
this leads to significant savings in data storage.  For example for a
$1100\times 1100$ matrix the storage saving is significant:-
\vskip 1pt
\noindent
\ttfamily
\begin{lstlisting}
full      1100x1100          9680000	double
sparse    1100x1100          5004	    double
\end{lstlisting}
\rmfamily
%\vskip 2pt
\noindent
For full matrices, MATLAB stores every matrix element internally.   Zero-valued
elements require the same amount of storage space as any other matrix element.
For large matrices with a high percentage of zero-valued elements, this scheme
significantly reduces the amount of memory required for data storage.

\newpage
\section{Reordering and Matrix Bandwidth}
%\noindent
%{\bf\underline {4. Reordering and Matrix Bandwidth}}
%\vskip 10pt
%\noindent
During the process of matrix factorization into a product by methods such as
Gaussian elimination and Cholesky factorization, there will always be a problem
with fill-in that is generating additional nonzero elements during the
elimination process (i.e. row operation).   Hence we are unable to take
advantage of the sparsity feature of the original matrix and end up wasting both
storage and computational time.  Therefore the problem is how to perform row
operations whilst minimizing the fill-in of nonzero elements.  One way to
achieve this is to try to position the nonzero elements near the main or
principal diagonal, so that reordering of rows and columns will not result in
additional nonzero elements.  The process of moving the nonzero element near the
principal diagonal is the same as reducing the profile or the bandwidth of the
matrix.

\vskip 5pt
\noindent
Consider an $n \times n$ matrix $A=(a_{ij})$.  If all matrix elements are zero
outside a diagonally confined band whose range is determined by constants $p$
and $q$;

\vskip 7pt
\noindent
\phantom{.} \hskip 20mm $a_{ij}=0$   $\text{if}$   $j<i-p$   $\text{or}$ $j>i+q$; $p,q \geq 0$
\vskip 7pt
\noindent
(or $j-i>p$, or $i-j>q$; $p, q \geq 0$ ), where $p$ and $q$ are the distances
above and below the main diagonal, then the bandwidth  $bw$ is given by
$bw=p+q+1$ (in other words, it is the smallest number of adjacent diagonals to
which the non-zero elements are confined).
\vskip 6pt
\noindent
A matrix is called a band matrix or banded matrix if its bandwidth is reasonably
small.
\vskip 6pt
\noindent
A band matrix with $p=q=0$ is a diagonal matrix; $
A=
\begin{bmatrix}
1 & 0 & 0 & 0  \\
0 & 1 & 0 & 0  \\
0 & 0 & 1 & 0  \\
0 & 0 & 0 & 1
\end{bmatrix}
$
\vskip 8pt
\noindent
A band matrix with $p=q=1$ is a tridiagonal matrix; $
B=
\begin{bmatrix}
1 & 1 & 0 & 0  \\
1 & 1 & 1 & 0  \\
0 & 1 & 1 & 1  \\
0 & 0 & 1 & 1
\end{bmatrix}
$
\vskip 8pt
\noindent
The indices of the nonzero elements for matrix $B$ are given in the following
table, using the conventional numbering used in MATLAB.
\vskip 10pt
\begin{center}
    \begin{tabular}{ c  c  c }
    $i$   &$j$   &$[i,j]$\\
    \hline
     1    & 1    & [1,1] \\
     2    & 1    & [2,1] \\
     1    & 2    & [1,2] \\
     2    & 2    & [2,2] \\
     3    & 2    & [3,2] \\
     2    & 3    & [2,3] \\
     3    & 3    & [3,3] \\
     4    & 3    & [4,3] \\
     3    & 4    & [3,4] \\
     4    & 4    & [4,4] \\
    \end{tabular}
\end{center}
\noindent
\vskip 5pt
\noindent
Therefore, the difference between i and j for each nonzero entry is either 0 or
1; hence the maximum distance is 1, i.e. in MATLAB the bandwidth calculation for
a tridiagonal matrix $T$ is performed by the following commands:-
\vskip 2pt
\noindent
\begin{lstlisting}
% Calculate bandwidth of a given matrix

>> T =[
     1     1     0     0
     1     1     1     0
     0     1     1     1
     0     0     1     1]

>> [i,j]=find(T);
>> bw=max(i-j)

bw =
     1
\end{lstlisting}
\vskip 12pt
\noindent
Note:
\vskip 4pt
\noindent

\begin{lstlisting}
[i,j]=find(T); %returns the row and column indices for
               % the nonzero entries
bw=max(i-j)    %returns an array the same size as i
               %and j with the largest elements taken
               %from i or j.
               %the dimensions of i and j must match.
\end{lstlisting}
\vskip 2pt
\noindent
Note - in some textbooks and articles the bandwidth is given as $bw=p+q+1$ - the
bandwidth of a tridiagonal matrix is given as 3 (i.e. $1(L) + 1(U) + 1(D)$). 
Also, when $p=q=2$ one has a pentadiagonal matrix and so on. Also, an upper
triangular matrix is obtained when $q=0$, $q=n-1$, and similarly, with $p=n-1$,
$q=0$, a lower triangular matrix is obtained.
%\vskip 2pt
%\noindent
\newpage
%{\bf\underline {4.1 Cuthill and McKee Reordering}}
%\vskip 10pt
%\noindent
\subsection{Cuthill and McKee Reordering}
The most simple and popular method of minimizing the bandwidth was developed by
Cuthill and McKee (1969).  Later the method was improved by Alan George who
developed the Reverse Cuthill and McKee (RCM) method which is much more
effective than the original CM algorithm.  There are several other efficient
bandwidth reduction methods used in re-ordering of sparse matrices, namely
Minimum Degree Reordering method.
\vskip 12pt
\noindent
The Cuthill and McKee algorithm is outlined in the following steps:-
\vskip 12pt
\noindent
\begin{enumerate}
   \item For a given sparse symmetric matrix, plot the adjacency graph
       (alternatively if you have been given the adjacency graph, then it is
       useful to write down the corresponding adjacency matrix).
\vskip 2pt
   \item Look at all the nodes, and produce a table listing the node numbers and
       the number of connections (or degree).
\vskip 2pt
   \item Select the node with the lowest degree (say $n_1$) and write it down
       under a column heading `Result/Head'.  If there is more than one node
       with the same number of degree, then choose one of them and write it down.
\vskip 2pt
   \item Look at the nodes connected directly to node $n_1$, write down in the
       next column `Queue' the nodes that are connected to $n_1$, in the order
       with lowest degree (e.g. $n_2$, $n_3$, $n_4$) that has not appeared in
       the Result column before.
\vskip 2pt
   \item Extract the first node in the queue - say $n_2$, if $n_2$ has not been
       previously inserted in `Result' then write $n_2$ below $n_1$ and check
       again for the connections.  Since $n_4$ is already in the queue, you need
       to see if $n_2$ is connected to any other nodes, say $n_5$ write down in
       the queue column $n_4$ if $n_2$ was not connected to a new node, just
       leave a blank in the queue column.
\vskip 2pt
   \item Now, write down $n_3$ below $n_2$ in the Results column, and repeat
       step 4.
\vskip 2pt
   \item Repeat steps 4 to 6 until there are no more nodes left to be
       considered.
\vskip 2pt
   \item Look at your list in the column Result - and renumber these as new node
       numbers.
\vskip 2pt
   \item Continue until the size of the array Result is $n$ for an $n\times n$
       matrix (i.e. all nodes are included in the array).
 \end{enumerate}
\vskip 6pt
\noindent
Note:- If the selected node in column Results is not connected to any other
node, then we put a `-' in the corresponding position in the queue column.

\newpage
\vskip 8pt
\noindent
{\bf\underline {Example 7.1}}
\vskip 7pt
\noindent
Consider the following sparse matrix and its corresponding adjacency plot:
\vskip 2pt
\noindent
\begin{table}[H]
  \begin{minipage}[b]{0.50\linewidth}
    \vskip 5pt
    \begin{center}
      $A=
      \begin{bmatrix}
        1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\
        0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 0 & 1
      \end{bmatrix}$
    \end{center}
    \vspace{10mm}
  \end{minipage}
  \begin{minipage}[b]{0.49\linewidth}
    \begin{center}
      \includegraphics[width=5cm]{main/07/Fig7a.tikz}
    \end{center}
  \end{minipage}
\end{table}
\begin{center}
  \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
  \begin{tabular}{ P{2cm} | P{2cm} P{2cm} P{2cm} P{2cm} P{2cm} }
    \small Original nodes &
    \small No. of connections &
    \small Results/ Head &
    \small \phantom{\rule{1.9cm}{1pt}} Queue &
    \small \phantom{\rule{1.9cm}{1pt}} RCM &
    \small \phantom{\rule{1.9cm}{1pt}} New nodes\\
    \hline\normalsize
    1             & 1              & 1          & 5    &7    &1        \\
    2             & 3              & 5          & 3    &4    &2        \\
    3             & 2              & 3          & 2    &8    &3        \\
    4             & 1              & 2          & 6,8  &6    &4        \\
    5             & 2              & 6          & -    &2    &5        \\
    6             & 2              & 8          & -    &3    &6        \\
    7             & 1              & 4          & 7    &5    &7        \\
    8             & 2              & 7          & -    &1    &8        \\
  \end{tabular}
\end{center}
%\vskip 5pt
\noindent
Note that the labels of new nodes are the reverse of the elements in the array
Results.  See below the re-ordered matrix and the adjacency graph (note the
diagram remains unchanged, but nodes are now relabelled).
\vskip 5pt
\noindent
\begin{table}[H]
  \begin{minipage}[b]{0.50\linewidth}
    \vskip 5pt
    \begin{center}
      $B=
      \begin{bmatrix}
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 & 1
      \end{bmatrix}$
    \end{center}
    \vspace{5mm}
  \end{minipage}
  \begin{minipage}[b]{0.49\linewidth}
    \begin{center}
      \includegraphics[width=5cm]{main/07/Fig7b.tikz}
    \end{center}
  \end{minipage}
\end{table}
\vskip -12pt
\noindent
It can be seen that the algorithm keeps all nonzero elements close to the
diagonal.  You can see visibly that there are more zero elements below and above
the main diagonal and nonzero elements are close to the main diagonal and the
re-ordered matrix exhibits a banded structure.
\vskip 6pt
\noindent
Using MATLAB the bandwidths of the original matrix A and the reordered matrix B
can be found as:-
\vskip 12pt
\noindent
\ttfamily
\begin{lstlisting}
>> [i,j]=find(A); %returns the row and col indices
                  %of the nonzero entries
>> bw=max(i-j)
bw =6

>> [i,j]=find(B);
>> bw=max(i-j)
bw =2
\end{lstlisting}
\rmfamily
\vskip 12pt
\noindent
i.e. The (half) bandwidth of the matrix has been reduced to 2 instead of 6;
hence, the full bandwidth of the matrices A and B are 13 and 5 respectively.
\vskip 12pt
\noindent
Note that the relabeling described above used the reverse of the array RCM of
the reordered nodes, hence this is the Reverse Cuthill-McKee (RCM) re-ordering
technique.  In the original Cuthill-McKee (CM) algorithm, the elements of array
Results are relabelled without reversing the elements. i.e.
\vskip 10pt
\noindent
\begin{center}
    \begin{tabular}{ r | l l l l l l l l l l }
    Original nodes  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    \hline
    Cuthill-McKee   & 1 & 5 & 3 & 2 & 6 & 8 & 4 & 7 \\
    Reverse CM      & 7 & 4 & 8 & 6 & 2 & 3 & 5 & 1 \\
    \end{tabular}
\end{center}
\noindent
\vskip 10pt

%{\bf\underline {4.2 MATLAB Commands}}
\noindent
In MATLAB, the command $symrcm(A)$ returns the symmetric reverse Cuthill-McKee
ordering of A. This is a permutation matrix p such that $A(p,p)$ tends to have
its nonzero elements closer to the main diagonal.
\vskip 6pt
\noindent
Example MATLAB code:
\noindent
\begin{lstlisting}
p = symrcm(A);
spy(A(p,p)), title('A(p,p) after Cuthill-McKee ordering')
nz = nnz(A);
pct = 100/numel(A);
xlabel(sprintf('nonzeros=%d (%.1f%%)',nz,nz*pct));
\end{lstlisting}

%\vspace{5mm}
\begin{figure}[H]
  \begin{center}
      \scriptsize\includegraphics[width=6cm]{main/07/example7_1fig1.tikz}
  \end{center}
\end{figure}

\newpage
\noindent
\rmfamily
Figure 5 below shows the nonzero pattern of the original matrix A, the ordered
matrix B, matrix C, ordered by the CM method, and matrix D is found by using the
MATLAB function $\emph{symrcm}$:-
\begin{figure}[H]
  \begin{center}
    \tiny
    \begin{tabular}{ccc}
      \includegraphics[width=6cm]{main/07/example7_1sub1.tikz}&&
      \includegraphics[width=6cm]{main/07/example7_1sub2.tikz}\\
      \includegraphics[width=6cm]{main/07/example7_1sub3.tikz}&&
      \includegraphics[width=6cm]{main/07/example7_1sub4.tikz}\\
    \end{tabular}
  \end{center}
\end{figure}
\vskip 10pt
Note that the half bandwidth of the reordered matrix $C$, using the original
Cuthill-Mckee method is $bw =5$, and the half bandwidth of the reordered matrix
$D$ is $bw =2$, using the MATLAB function $sysmrcm$ for the reverse
Cuthill-Mckee is $bw =2$, i.e. the same as that for matrix $B$. Hence, the
Reverse Cuthill-Mckee algorithm for bandwidth reduction of sparse matrices seems
to be the most effective reordering method.
\vskip 6pt
\noindent
\newpage
\rmfamily
\vskip 12pt
\noindent
%{\bf\underline {4.2 Column Count Reordering}}
\subsection{Column Count Reordering}
\vskip 10pt
\noindent
The idea of the column count reordering algorithm is to move rows and columns
with higher nonzero count towards the end of the matrix. This algorithm reduces
the time and storage for computing matrix operations such as the LU
factorization and the Cholesky factorization, but its performance is not
consistent overall and depends on the original matrix.
\vskip 10pt
\noindent
{\bf\underline {Example 7.2}}
\vskip 2pt
\noindent
Consider the sparse matrix and its corresponding adjacency graph from Example
7.1:
\vskip 4pt
\noindent
\begin{table}[H]
  \begin{minipage}[b]{0.50\linewidth}
    \vskip 2pt
    \begin{center}
      $A=
      \begin{bmatrix}
        1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\
        0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 0 & 1
      \end{bmatrix}$
    \end{center}
    \vspace{5mm}
  \end{minipage}
  \begin{minipage}[b]{0.49\linewidth}
    \begin{center}
      \includegraphics[width=5cm]{main/07/Fig7a.tikz}
    \end{center}
  \end{minipage}
\end{table}
\vskip -10pt
The degrees of all vertices are obtained in the following:
\vskip 10pt
\noindent
\begin{center}
  \begin{tabular}{ l | l l l l l l l l }
    vertex          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    \hline
    no. of degree   & 1 & 3 & 2 & 1 & 2 & 2 & 1 & 2 \\
  \end{tabular}
\end{center}
\rmfamily
\noindent
\vskip 5pt
Since vertex 7 has the least degrees, it is labelled as 1 - (Note: since
vertices 1, 4, and 7 have the same number of degrees, therefore the new labels
are interchangeable, as any of these vertices can be chosen in any order) and
then vertices 4 and 1 are labelled (again since vertices 4 and 1 have the same
number of degrees, therefore the new labels are interchangeable). The algorithm
is then continued in the same steps until complete labelling is obtained as
shown:
\noindent
  \begin{center}
    \begin{tabular}{ l | l l l l l l l l }
    new vertex   & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    \hline
    old vertex   & 7 & 4 & 1 & 8 & 6 & 5 & 3 & 2 \\
  \end{tabular}
\end{center}
\noindent
\vskip 7pt
The resulting adjacency matrix and graph are shown below:
\begin{table}[H]
  \begin{minipage}[b]{0.49\linewidth}
    \vskip 5pt
    \begin{center}
      $CC=
      \begin{bmatrix}
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\
        0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\
        0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 0 & 1 & 1
      \end{bmatrix}$
    \end{center}
    \vspace{5mm}
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \includegraphics[width=5cm]{main/07/F7CCalog.tikz}
  \end{minipage}
\end{table}
\noindent
Similar to the command $\emph{symrcm}$ for RCM reordering, in MATLAB the command
$\emph{colperm}$ returns the column count reordering of A. This is a permutation
matrix q such that $A(q,q)$ tends to move rows and columns with higher nonzero
count towards the end of the matrix (a down-arrow effect).

Example MATLAB code:
\begin{lstlisting}
q = colperm(A);
spy(A(q,q)),title('A(q,q)-Column Count ordering by colperm')
nz = nnz(A);
pct = 100 / numel(A);
xlabel(sprintf('nonzeros=%d (%.1f%%)',nz,nz*pct));
\end{lstlisting}
\begin{figure}[H]
  \begin{center}
    \scriptsize
    \begin{tabular}{ccc}
      \includegraphics[width=0.4\textwidth]{main/07/cc_comb_1.tikz}
      &&
      \includegraphics[width=0.4\textwidth]{main/07/cc_comb_2.tikz}
    \end{tabular}
  \end{center}
\end{figure}

\subsection{Minimum Degree Reordering}
The minimum degree algorithm attempts to reduce the fill-in caused by
elimination such as Gaussian elimination, LU decomposition or the Cholesky
decomposition. The computational costs of this algorithm is inexpensive, and it
is easy to apply and effective for most problems which are not too large.
However, most mathematical software packages such as MATLAB use an approximation
minimum degree, rather than the exact degree to minimise the computational costs.
The minimum degree algorithm is re-labelling of the vertices in ascending degree
order. Nodes with lower degrees are labelled early to reduce adding many
fill-ins.

\newpage
{\bf\underline {Example 7.3}}
\vskip 2pt
\noindent
Consider the sparse matrix and its corresponding adjacency graph from Example
7.1:
%\vskip 6pt
\noindent
\begin{table}[H]
  \begin{minipage}[]{0.50\linewidth}
    \begin{center}
      $A=
      \begin{bmatrix}
        1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\
        0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0 & 0 & 1 & 0 & 1
     \end{bmatrix}$
    \end{center}
    %\vspace{2mm}
  \end{minipage}
  \begin{minipage}[]{0.49\linewidth}
    \begin{center}
      \includegraphics[width=4cm]{main/07/Fig7a.tikz}
    \end{center}
  \end{minipage}
\end{table}

The minimum degree algorithm is re-labelling of the vertices in ascending degree
order. Nodes with lower degrees are labelled early to reduce adding many
fill-ins.

The degrees of all vertices are shown in the following table:
%\vskip 2pt
%\noindent
    \begin{center}
        \begin{tabular}{ r | l l l l l l l l }
        vertex          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
        \hline
        no. of degree   & 1 & 3 & 2 & 1 & 2 & 2 & 1 & 2 \\
    \end{tabular}
\end{center}
%\vskip 10pt
%\noindent
where vertex 7 has the least degrees, then vertex 7 is numbered with first as 1.
Since vertex 7 is connected to vertex 4, then vertex 4 must be updated by
subtracting 1 (if it is already 1 then just write it down as 1 and number it
immediately after 7) as shown in the following Table - where the eliminated node
is represented by X , and the vertex is numbered with a new label. Note: if no
tie-breaking strategy has been established, all three vertices 1, 4, and 7 can
be selected in this case, and proceed to elimination. The completed ordering
using the Minimum Degree algorithm is shown in the following table, together
with the resulting adjacency matrix and graph:
%\vskip 2pt
%\noindent
\begin{center}
    \begin{tabular}{cc|cccccccc}
        %\hline
        \multicolumn{2}{p{1.5cm}|}{} &
        \multicolumn{8}{c}{vertex and no. of degree} \\
        %\cline{3-10}
        \small
            &               & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
            &New ordering   & 1 & 3 & 2 & 1 & 2 & 2 & 1 & 2 \\
        \hline
         1  &(7)            & 1 & 3 & 2 & 1 & 2 & 2 & X & 2 \\
         2  &(4)            & 1 & 3 & 2 & X & 2 & 2 & X & 2 \\
         3  &(1)            & X & 3 & 2 & X & 1 & 2 & X & 2 \\
         4  &(5)            & X & 3 & 1 & X & X & 2 & X & 2 \\
         5  &(3)            & X & 2 & X & X & X & 2 & X & 2 \\
         6  &(6)            & X & 1 & X & X & X & X & X & 1 \\
         7  &(8)            & X & 1 & X & X & X & X & X & X \\
         8  &(2)            & X & X & X & X & X & X & X & X \\
    \end{tabular}
\end{center}
%\noindent

%\begin{table}[H]
  \begin{minipage}[]{0.50\linewidth}
    %\vskip 5pt
    \begin{center}
      $MD=
      \begin{bmatrix}
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
        0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
        0 & 0 & 0 & 0 & 1 & 1 & 1 & 1
      \end{bmatrix}$
    \end{center}
    %\vspace{5mm}
  \end{minipage}
  \begin{minipage}[]{0.49\linewidth}
    \begin{center}
    \includegraphics[width=4cm]{main/07/F7MDalgo.tikz}
    \end{center}
  \end{minipage}
%\end{table}

%\newpage
%\noindent
Similar to the commands $\emph{symrcm}$ and $\emph{colperm}$, in MATLAB the
command $\emph{symamd}$ returns the approximate Minimum Degree reordering of A
to produce large blocks of zeros in the matrix. The permutation matrix r is
calculated such that $A(r,r)$ tends to move rows and columns with higher nonzero
elements towards the end of the matrix - similar to the Column Count algorithm.
%\vskip 10pt

Example MATLAB code:
%\vskip 2pt
{%\small
\begin{lstlisting}
r = symamd(A);
spy(A(r,r)),title('A(r,r)-Minimum Degree ordering by symamd')
nz = nnz(A);
pct = 100 / numel(A);
xlabel(sprintf('nonzeros=%d (%.1f%%)',nz,nz*pct));
\end{lstlisting}}
\begin{figure}[H]
  \begin{center}
    \scriptsize
    \begin{tabular}{ccc}
      \includegraphics[width=0.4\textwidth]{main/07/MD_comb_1.tikz}
      &&
      \includegraphics[width=0.4\textwidth]{main/07/MD_comb_2.tikz}
    \end{tabular}
  \end{center}
\end{figure}
%\noindent
%\vskip 10pt
%\noindent
%%{\bf\underline {4.4 Comparison between various ordering schemes and fill-in effect}}
%\vskip 10pt
%\noindent

%\newpage

\subsection{Comparison between various ordering schemes and fill-in effect}
For a more effective comparison we consider a symmetric $20 \times 20$ sparse
matrix A, which was generated by using MATLAB's {\it{rand}} command:
\begin{lstlisting}
>> S = sparse(+(rand(20,20) < 1/3)); % should allow <10% nonzeros
>> B=full(S);
>> A=B'*B  % makes A a symmetric matrix
\end{lstlisting}

Using the MATLAB $\emph{spy}$ command the corresponding sparsity patterns from
the results of the reordering by the RCM, CC and MD methods are summarised in
Figure 6.  Furthermore, using the MATLAB command $[L,U] = lu(A)$ the upper
triangular matrix in $U$ for the full matrix A and the reordered matrices are
also shown in Figure 6. [ Note: for the original matrix $bw =18$ and for the RCM
reordered matrix $bw =7$ were found using MATLAB]
\newpage

\begin{figure}[H]
  \begin{center}
    \begin{tabular}{cc}
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a1.tikz} &
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a2.tikz}\\
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a3.tikz} &
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a4.tikz}\\
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a5.tikz} &
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a6.tikz}\\
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a7.tikz} &
      \scriptsize\includegraphics[width=0.35\textwidth]{main/07/fig6a8.tikz}\\
    \end{tabular}
  \end{center}
  Figure 6. Comparison between reordering schemes for a $20 \times 20$ symmetric matrix
\end{figure}
\vskip 10pt
\noindent

In summary, the re-ordering or permutation of matrices provides a
pre-conditioning for the solution of matrices and the resulting matrix is
already nearly in LU factorization or banded form depending on the chosen
ordering method. Applying methods for the solution of full matrices will produce
more nonzero elements (fill-in effect) and no computational advantage from the
sparsity of matrix can be achieved.
\vskip 10pt
\noindent
For the $20 \times 20$ matrix $A$ the number of fill-in required to form the
upper triangular using LU factorization of the permuted matrix is shown in the
following table:

\vskip 10pt
\noindent
\begin{center}
    \begin{tabular}{c|cccc}
        $20 \times 20$ & Original Matrix & Reverse CM & Column Count & Min Degree  \\ \hline
        No. of nz in U &$ 66 $           & $ 54 $      & $ 53 $        &$ 58 $  \\
    \end{tabular}
\end{center}
\vskip 10pt
\noindent

For large matrices with a high percentage of zero elements, reordering process
of matrices significantly reduces the number of fill-in and the CPU processing
time , as well as the amount of memory required for data storage when solving
large sparse linear system of equations.
\vskip 10pt
\noindent
For a $3000\times 3000$ matrix as an example the effect of reordering on the LU
factors are presented in the following table:

\vskip 10pt
\noindent
\begin{center}
  \begin{tabular}{c|cr|c}
    $3000 \times 3000$ & \multicolumn{2}{c|}{nnz (percentage)} &  CPU$(sec)$ \\ \hline
    Original Matrix    &$ 1783708$ &$(100)\%$& $5.500$ \\
    Reverse CM         &$ 862448$  &$(48)\% $& $1.438$ \\
    Column Count       &$ 521582$  &$(29)\% $& $1.016$ \\
    Minimum Degree     &$ 692034$  &$(39)\% $& $1.047$ \\
  \end{tabular}
\end{center}
\vskip 5pt
\noindent

\vskip 4pt
\noindent
{\bf\underline {Example 7.4}}
\vskip 2pt
\noindent
Consider the following sparse matrix and its corresponding adjacency graph.
\vskip 1pt
\noindent
\begin{table}[H]
  \begin{minipage}[b]{0.5\linewidth}
    \vskip 2pt
		%\begin{center}
    $A=
		\begin{bmatrix}
      1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\
      1 & 1 & 0 & 0 & 1 & 1 & 1 & 0 \\
      0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\
      0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 & 1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
      1 & 1 & 1 & 0 & 1 & 0 & 1 & 0 \\
      1 & 0 & 1 & 0 & 0 & 0 & 0 & 1
    \end{bmatrix}$
		%\end{center}
    \vspace{-5mm}
  \end{minipage}
  \begin{minipage}[b]{0.59\linewidth}
	%\begin{center}
    \includegraphics[width=6cm]{main/07/ex74_2.tikz}
		%\end{center}
  \end{minipage}
\end{table}
\vskip 10pt

For the RCM and Column Count reordering methods, use the MATLAB commands
$symrcm(A)$ and $colperm(A)$ to find the corresponding reordered matrices $C$
and $D$. Plot the pattern of matrices $A$, $C$ and $D$. Apply the MATLAB command
$[L,U]=lu(A)$ for finding the LU factorisation of matrices $A$, and similarly $C$
and $D$. Plot the pattern of the resulting matrices using $spy(lu(A))$,
$spy(lu(C))$ and $spy(lu(D))$. Comment on the number of non-zero elements
following LU factorisation. Plot the upper triangular matrix U for each of the
factorised matrices and record the number of non-zero (nz) elements in U for
each of the resulting upper triangular matrices  in a table. Comment on the
advantages of the LU factorisation and the effect of fill-in when using ordered
matrices.
\rmfamily
The three stages of the required solution together with the MATLAB commands are
summerised in Figure 7.
%\newpage

\begin{figure}[H]
  \begin{center}
    \begin{tabular}{ccc}
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_orig1.tikz} &
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_rcm1.tikz}  &
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_col1.tikz} \\
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_orig2.tikz} &
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_rcm2.tikz}  &
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_col2.tikz} \\
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_orig3.tikz} &
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_rcm3.tikz}  &
      \scriptsize\includegraphics[width=0.3\textwidth]{main/07/image_col3.tikz} \\
    \end{tabular}
  \end{center}
  \vskip 10pt
  Figure 7. Comparison of the LU factorisation and fill-in effect on original
  and ordered matrices.
\end{figure}

\vskip 10pt

\noindent
Following LU factorisation, we find the number of non-zero elements in various
reordering methods in $U$ as:

\vskip 5pt
\noindent
\begin{center}
  \begin{tabular}{c|ccc}
    $8 \times 8$ & Original Matrix & Reverse CM & Column Count  \\ \hline
    No. of nz in U     &$ 24 $           & $ 17 $      & $ 18 $ \\
  \end{tabular}
\end{center}
\vskip 5pt
\noindent
Hence, using the re-ordering methods, both RCM and Column Count, significantly
reduces the generation of fill-in elements compared to the LU factorisation of
the original matrix $A$, even for an $8\times8$ matrix.

\noindent
Furthermore, the solution to the original linear algebraic system contributes
significantly to the total computational time for the solution of stiff
problems, as well as affecting the accuracy of the solution (and hence,
affecting the computational time). Note that re-ordering enhances the
computational accuracy through a reduction in rounding errors (due to fewer
computational steps, resulting from reduced fill-in elements). In terms of
application in the solutions of large systems of stiff differential equations
(which require treatment of the associated Jacobinan metrix), the re-ordering of
the system of equations resulting from the Newton iteration will lead to the
solution of a banded matrix instead of a full matrix treatment, and hence
resulting in fewer fill-ins, more accurate solutions and shorter computing time.

\vskip 20pt
\noindent
{\bf\underline {References}}
\vskip 10pt
\noindent
MATLAB Help/documentation on Sparse Matrices, e.g.:

http://www.mathworks.co.uk/help/matlab/math/accessing-sparse-matrices.html

http://www.mathworks.co.uk/help/matlab/examples/sparse-matrices.html


\noindent
Saad, Y., 2003. Iterative Methods for Sparse Linear System 2nd ed. Philadelphia:
SIAM- Society for Industrial and Applied Mathematics

\noindent
Golub, G. and Van Loan, C., 1996g. Matrix Computations 3rd edition. London: The
Johns Hopkins University Press

\newpage

{\bf\underline {Exercise (7)}}

\vskip 4pt
\noindent
\begin{enumerate}
\item (a) Write a Matlab mfile that produces the adjacency graph for the
symmetric matrix A given in Example 4. (b) Modify your mfile and repeat for the 
matrix A:
A=[1 1 0 0 0 1, 1 1 0 0 1 0, 0 0 1 0 1 0, 0 0 0 1 0 1, 0 1 1 0 1 0, 1 0 0 1 0 1].
\vskip 2pt
\item {Consider the following adjacency graph with 10 nodes:
\vskip -6pt
\begin{table}[H]
\hspace*{20mm} \includegraphics{main/07/ch7ex3.tikz}
\end{table}
\parskip -10pt
\noindent
Show the corresponding adjacency matrix $A$ and calculate its bandwidth using
the MATLAB command \emph{bandwidth}. Reorder the matrix $A$ using the Cuthill-McKee (CM)
algorithm and label the corresponding matrix $B$. Reorder the matrix $A$ using
the RCM algorithm and label the corresponding matrix C. Compare the bandwidths
of matrices $A$, $B$ and $C$ and comment on the computational efficiency when
using matrix re-ordering. Plot the sparsity pattern of matrices $A$, $B$, and
$C$, and verify your results by using the MATLAB function {\it symrcm(A)}.
Reorder the Matrix $A$ by Column Count (label $D$) and Minimum Degree (label
$M$) schemes, write down the corresponding adjacency matrices and plot the
adjacency graphs. For the three schemes RCM, CC and MD (i.e. matrices $C$, $D$
and $M$), perform LU factorisation and record the number of fill-in elements in
a table, prepare a summary result table (similar to Figure 6 shown in the
lecture notes) and comment.}
\vskip 4pt
\item Repeat question 2 with the following adjacency graph with 9 nodes.
\begin{figure}[H]
\hspace*{20mm}$\includegraphics{main/07/ch7ex2v2.tikz}$
\end{figure}
\parskip -10pt
\noindent
Calculate the storage in units of bytes for the full and sparse matrix $A$ and
the reordered Matrices $D$ (Column Count) and $M$ (Minimum Degree), and comment.
\end{enumerate}

\input{main/07/chap07_answers}